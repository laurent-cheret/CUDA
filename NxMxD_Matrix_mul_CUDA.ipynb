{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_CUDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z05zD47xu3a"
      },
      "source": [
        "BELOW IS THE PART OF THE CODE THAT IS NECESSARY TO RUN IN ORDER TO DOWNGRADE CUDA TO VERSION 9.2, THE NEWEST VERSION 11 DOESN'T WORK WITH THE NVCC PLUGIN FOR NOTEBOOKS. THIS IS ONLY EXECUTED IN THIS SPECIFIC NOTEBOOK SO THERE IS NO DANGER OF DOWNGRADNG ANYTHING IN THE LOCAL MACHINE. \n",
        "\n",
        "IF THE NOTEBOOK IS RESTARTED THEN THE VERSION GOES BACK TO 11."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDVf0qiSznp8"
      },
      "source": [
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfwKQl0Oym69"
      },
      "source": [
        "AFTER UNINSTALLING CUDA THE CODE BELOW REINSTALLS THE OLDER VERSION 9.2 THAT CAN WORK WITH THE NVCC PLUGIN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dyRhzGV1Kko"
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STtuYaneyvjF"
      },
      "source": [
        "LOADING THE NVCC PLUGING."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfwHSarqnlC-",
        "outputId": "d03e2d8b-58a2-4469-eb1c-97478bfe42db"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%reload_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-n425rv3k\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-n425rv3k\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4305 sha256=9487f8a61631c0af4105f94bf83f904afd5185b392ca830b65140084024d2fd9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-za_nwir4/wheels/c5/2b/c0/87008e795a14bbcdfc7c846a00d06981916331eb980b6c8bdf\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNQT-Umk1ZRY",
        "outputId": "08289e67-ad09-4ae9-f1dd-2a4ccf4ec0ba"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Wed_Apr_11_23:16:29_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGaXzwbDy1bt"
      },
      "source": [
        "HELLO WORLD CODE JUST TO CHECK IT IS WORKING."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwBxLrlI8JlO",
        "outputId": "ed830044-a84c-4b4d-e5c8-142dedef604d"
      },
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "\n",
        "__global__ void print_from_gpu(void) {\n",
        "    printf(\"Hello World! from thread [%d,%d]\", threadIdx.x,blockIdx.x);\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "\n",
        "    printf(\"Hello World from host!\");\n",
        "    print_from_gpu<<<2,1>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from host!Hello World! from thread [0,0]Hello World! from thread [0,1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njtCj97JIivW"
      },
      "source": [
        "FIRST CODE USING A FOR LOOP TO CALCULATE ONE MATRIX MULTIPLICATION AFTER THE OTHER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psOibprMeB1P",
        "outputId": "ca3cc1a8-622e-4eec-9533-41aa8a650a2d"
      },
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "__global__ void matrixmul(int *A, int* B, int* C, int dim_n, int dim_k){\n",
        "    //find row and column\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int column = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int p_sum = 0;\n",
        "    for (int i = 0; i < dim_n; i++){\n",
        "        p_sum += A[row * dim_n + i] * B[i * dim_k+ column];\n",
        "    }\n",
        "    C[row * dim_k + column] = p_sum;\n",
        "}\n",
        "\n",
        "void arrPrintMatrix(int *matrix, int m, int x) {\n",
        "    int i, j;\n",
        "\n",
        "    for (i = 0; i < m; i++) {\n",
        "        printf(\"\\n\");\n",
        "        for (j = 0; j < x; j++) {\n",
        "            printf(\"%d\\t\", matrix[i * x + j]);\n",
        "        }\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    clock_t begin = clock();\n",
        "    // THIS FOR LOOP IS FOR CALCULATING Cj ONE AT A TIME.\n",
        "    for (int N = 0; N < 100; N++){\n",
        "    // I CHOSE THE EXAMPLE IN WHICH THE MATRICES ARE A = 500x500 AND B = 500x400.    \n",
        "    int n = 500;\n",
        "    int k = 400;\n",
        "\n",
        "    // host matrix m,n,p\n",
        "    int* h_m;\n",
        "    int* h_n;\n",
        "    int* h_p;\n",
        "\n",
        "    //device matrix m,n,p\n",
        "    int* d_m;\n",
        "    int* d_n;\n",
        "    int* d_p;\n",
        "\n",
        "    size_t bytes = n*n*sizeof(int);\n",
        "\n",
        "    //allocate memory on host side\n",
        "\n",
        "    h_m = (int*)malloc(bytes);\n",
        "    h_n = (int*)malloc(bytes);\n",
        "    h_p = (int*)malloc(bytes);\n",
        "\n",
        "    //initialize matrix A\n",
        "    for (int i = 0; i < n; i++){\n",
        "        for (int j = 0; j < n; j++){\n",
        "            h_m[i*n + j] = rand() % 1024;\n",
        "\n",
        "        }\n",
        "    }\n",
        "\n",
        "    //initialize matrix B\n",
        "    for (int i = 0; i < n; i++){\n",
        "        for (int j = 0; j < k; j++){\n",
        "          \n",
        "            h_n[i*k + j] = rand() % 1024;\n",
        "\n",
        "        }\n",
        "    }\n",
        "\n",
        "    //arrPrintMatrix(h_m,n,n);\n",
        "    //arrPrintMatrix(h_n,n,k);\n",
        "\n",
        "    //allocate memory gpu\n",
        "    cudaMalloc(&d_m, bytes);\n",
        "    cudaMalloc(&d_n, bytes);\n",
        "    cudaMalloc(&d_p, bytes);\n",
        "\n",
        "    cudaMemcpy(d_m, h_m, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_n, h_n, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    int threads_per_block = 128;\n",
        "    dim3 block_size(threads_per_block, threads_per_block);\n",
        "    dim3 grid_size(n/block_size.x, n/block_size.y);\n",
        "    matrixmul <<<grid_size, block_size>>>(d_m, d_n, d_p, n,k);\n",
        "\n",
        "    cudaMemcpy(h_p, d_p, bytes, cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    //arrPrintMatrix(h_p, n,k);\n",
        "    }\n",
        "    clock_t end = clock();\n",
        "    double time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
        "    printf (\"Your calculations took %.2lf seconds to run.\\n\", time_spent );\n",
        "    printf(\"---------------------------==========--------------------------\");\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your calculations took 1.32 seconds to run.\n",
            "---------------------------==========--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF-ivam4y8Po"
      },
      "source": [
        "THIS CODE BELOW IS THE EXAMPLE IN WHICH MATRICES A AND B ARE 4x4 and 4x3, N IS 10. THIS IS TO SHOW THE CODE IS WORKING AND TO PRINT SOME EXAMPLES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0922te6PdwdH",
        "outputId": "b8082f8e-b8a4-4179-e383-a855e7bedde0"
      },
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "__global__ void matrixmul(int *A, int* B, int* C, int dim_n, int dim_k){\n",
        "    //find row and column\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int column = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int p_sum = 0;\n",
        "\n",
        "    if( (row < dim_n) && (column < dim_k)){\n",
        "    for (int i = 0; i < dim_n; i++){\n",
        "        p_sum += A[row * dim_n + i] * B[i * dim_k+ column];\n",
        "    }\n",
        "    C[row * dim_k + column] = p_sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "void arrPrintMatrix(int *matrix, int m, int x) {\n",
        "    int i, j;\n",
        "\n",
        "    for (i = 0; i < m; i++) {\n",
        "        printf(\"\\n\");\n",
        "        for (j = 0; j < x; j++) {\n",
        "            printf(\"%d\\t\", matrix[i * x + j]);\n",
        "        }\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    clock_t begin = clock();\n",
        "    // THIS FOR LOOP IS FOR CALCULATING Cj ONE AT A TIME.\n",
        "    for (int N = 0; N < 10; N++){\n",
        "    // I CHOSE THE EXAMPLE IN WHICH THE MATRICES ARE A = 5x5 AND B = 5x4.    \n",
        "    int n = 5;\n",
        "    int k = 4;\n",
        "\n",
        "    // host matrix m,n,p\n",
        "    int* h_m;\n",
        "    int* h_n;\n",
        "    int* h_p;\n",
        "\n",
        "    //device matrix m,n,p\n",
        "    int* d_m;\n",
        "    int* d_n;\n",
        "    int* d_p;\n",
        "\n",
        "    size_t bytes = n*n*sizeof(int);\n",
        "\n",
        "    //allocate memory on host side\n",
        "\n",
        "    h_m = (int*)malloc(bytes);\n",
        "    h_n = (int*)malloc(bytes);\n",
        "    h_p = (int*)malloc(bytes);\n",
        "\n",
        "    //initialize matrix A\n",
        "    for (int i = 0; i < n; i++){\n",
        "        for (int j = 0; j < n; j++){\n",
        "            h_m[i*n + j] = rand() % 1024;\n",
        "\n",
        "        }\n",
        "    }\n",
        "\n",
        "    //initialize matrix B\n",
        "    for (int i = 0; i < n; i++){\n",
        "        for (int j = 0; j < k; j++){\n",
        "          \n",
        "            h_n[i*k + j] = rand() % 1024;\n",
        "\n",
        "        }\n",
        "    }\n",
        "\n",
        "    arrPrintMatrix(h_m,n,n);\n",
        "    arrPrintMatrix(h_n,n,k);\n",
        "\n",
        "    //allocate memory gpu\n",
        "    cudaMalloc(&d_m, bytes);\n",
        "    cudaMalloc(&d_n, bytes);\n",
        "    cudaMalloc(&d_p, bytes);\n",
        "\n",
        "    cudaMemcpy(d_m, h_m, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_n, h_n, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    int threads_per_block = 8;\n",
        "    dim3 block_size(threads_per_block, threads_per_block);\n",
        "    //dim3 grid_size(n/block_size.x, n/block_size.y);\n",
        "    dim3 grid_size(1,1);\n",
        "    matrixmul <<<grid_size, block_size>>>(d_m, d_n, d_p, n,k);\n",
        "\n",
        "    cudaMemcpy(h_p, d_p, bytes, cudaMemcpyDeviceToHost);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaFree(d_m);\n",
        "    cudaFree(d_n);\n",
        "    cudaFree(d_p);\n",
        "    arrPrintMatrix(h_p, n,k);\n",
        "    }\n",
        "    clock_t end = clock();\n",
        "    double time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
        "    printf (\"Your calculations took %.2lf seconds to run.\\n\", time_spent );\n",
        "    printf(\"---------------------------==========--------------------------\");\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "359\t966\t105\t115\t81\t\n",
            "255\t74\t236\t809\t205\t\n",
            "186\t939\t498\t763\t483\t\n",
            "326\t124\t706\t84\t1016\t\n",
            "795\t488\t487\t909\t886\t\n",
            "\n",
            "346\t302\t611\t563\t\n",
            "927\t201\t922\t870\t\n",
            "306\t13\t951\t561\t\n",
            "88\t163\t346\t293\t\n",
            "349\t261\t791\t88\t\n",
            "\n",
            "1090215\t343835\t1313717\t1142265\t\n",
            "371781\t280324\t890538\t595418\t\n",
            "1322908\t501817\t2099053\t1467089\t\n",
            "805756\t411422\t1817640\t801504\t\n",
            "1265674\t723922\t2414158\t1489657\t\n",
            "\n",
            "745\t94\t212\t427\t178\t\n",
            "205\t198\t667\t692\t84\t\n",
            "529\t14\t386\t116\t577\t\n",
            "289\t317\t476\t135\t624\t\n",
            "489\t62\t161\t577\t225\t\n",
            "\n",
            "508\t871\t574\t769\t\n",
            "638\t663\t490\t732\t\n",
            "875\t918\t911\t56\t\n",
            "92\t554\t748\t176\t\n",
            "59\t763\t562\t175\t\n",
            "\n",
            "673718\t1278205\t1086254\t759887\t\n",
            "882709\t1369595\t1387151\t476425\t\n",
            "660129\t1328904\t1073194\t560056\t\n",
            "814794\t1449760\t1206520\t613901\t\n",
            "495202\t1106156\t1015783\t571368\t\n",
            "\n",
            "316\t852\t492\t792\t987\t\n",
            "92\t258\t26\t254\t835\t\n",
            "251\t762\t682\t826\t507\t\n",
            "297\t465\t998\t5\t316\t\n",
            "892\t916\t373\t984\t446\t\n",
            "\n",
            "97\t137\t505\t860\t\n",
            "699\t680\t153\t527\t\n",
            "149\t945\t491\t241\t\n",
            "179\t517\t495\t1015\t\n",
            "768\t233\t673\t570\t\n",
            "\n",
            "1599292\t1727027\t1587799\t2205806\t\n",
            "879886\t538487\t786385\t955112\t\n",
            "1195833\t1742210\t1328284\t1909176\t\n",
            "746129\t1376212\t926291\t926188\t\n",
            "1301049\t1710215\t1560989\t2592725\t\n",
            "\n",
            "741\t970\t11\t715\t976\t\n",
            "328\t583\t868\t701\t543\t\n",
            "291\t798\t680\t796\t635\t\n",
            "356\t453\t788\t883\t602\t\n",
            "709\t350\t843\t889\t867\t\n",
            "\n",
            "315\t880\t612\t548\t\n",
            "529\t158\t265\t476\t\n",
            "170\t980\t428\t498\t\n",
            "539\t272\t175\t59\t\n",
            "563\t973\t739\t336\t\n",
            "\n",
            "1683288\t1960248\t1561639\t1243387\t\n",
            "1242835\t1950405\t1250687\t1113323\t\n",
            "1415956\t1882931\t1289167\t1138280\t\n",
            "1300600\t1983016\t1274584\t1057509\t\n",
            "1519087\t2590759\t1683750\t1318709\t\n",
            "\n",
            "584\t71\t789\t348\t955\t\n",
            "367\t34\t281\t186\t923\t\n",
            "125\t501\t779\t737\t26\t\n",
            "284\t895\t291\t760\t41\t\n",
            "248\t164\t539\t787\t437\t\n",
            "\n",
            "714\t846\t1000\t664\t\n",
            "562\t312\t224\t633\t\n",
            "77\t573\t564\t444\t\n",
            "607\t846\t631\t506\t\n",
            "971\t108\t261\t684\t\n",
            "\n",
            "1656172\t1365861\t1513743\t1612343\t\n",
            "1311918\t739143\t891369\t1115422\t\n",
            "903400\t1334739\t1148413\t1136715\t\n",
            "1229304\t1333635\t1138865\t1296919\t\n",
            "1212779\t1282821\t1199386\t1204930\t\n",
            "\n",
            "134\t545\t555\t426\t282\t\n",
            "597\t674\t446\t112\t437\t\n",
            "883\t827\t260\t860\t467\t\n",
            "822\t148\t691\t431\t226\t\n",
            "240\t996\t670\t847\t818\t\n",
            "\n",
            "277\t329\t765\t386\t\n",
            "590\t425\t520\t112\t\n",
            "980\t946\t394\t553\t\n",
            "596\t840\t666\t10\t\n",
            "700\t469\t270\t536\t\n",
            "\n",
            "1353864\t1290839\t964436\t575091\t\n",
            "1372761\t1203812\t1175491\t787920\t\n",
            "1826781\t1829365\t1906825\t836154\t\n",
            "1407270\t1455058\t1326110\t841437\t\n",
            "2388132\t2231202\t1750462\t1021620\t\n",
            "\n",
            "936\t68\t684\t603\t499\t\n",
            "910\t844\t471\t557\t667\t\n",
            "265\t834\t997\t6\t196\t\n",
            "563\t431\t717\t675\t388\t\n",
            "639\t45\t941\t212\t886\t\n",
            "\n",
            "583\t222\t562\t28\t\n",
            "492\t74\t964\t560\t\n",
            "758\t544\t35\t645\t\n",
            "364\t507\t178\t7\t\n",
            "772\t1012\t1004\t779\t\n",
            "\n",
            "1702336\t1395629\t1223854\t898410\t\n",
            "2020468\t1478103\t2110335\t1325407\t\n",
            "1474045\t864308\t1185653\t1270251\t\n",
            "1629003\t1281809\t1266687\t1026566\t\n",
            "1869115\t1661208\t1362713\t1341715\t\n",
            "\n",
            "185\t544\t186\t902\t195\t\n",
            "574\t517\t241\t492\t729\t\n",
            "103\t51\t951\t665\t80\t\n",
            "419\t739\t20\t979\t473\t\n",
            "564\t1015\t94\t928\t498\t\n",
            "\n",
            "272\t936\t246\t261\t\n",
            "916\t1\t446\t436\t\n",
            "188\t324\t632\t762\t\n",
            "841\t873\t230\t547\t\n",
            "976\t282\t474\t617\t\n",
            "\n",
            "1532494\t1076404\t705576\t1040910\t\n",
            "1800284\t1250959\t982804\t1277785\t\n",
            "890865\t1007688\t839986\t1186896\t\n",
            "2079639\t1387456\t894680\t1274157\t\n",
            "2367316\t1509955\t1100334\t1476254\t\n",
            "\n",
            "362\t894\t332\t382\t849\t\n",
            "805\t947\t840\t900\t851\t\n",
            "314\t148\t763\t561\t409\t\n",
            "656\t562\t855\t68\t750\t\n",
            "155\t700\t489\t997\t549\t\n",
            "\n",
            "719\t520\t501\t1001\t\n",
            "994\t94\t339\t864\t\n",
            "426\t722\t690\t208\t\n",
            "645\t506\t84\t472\t\n",
            "821\t232\t212\t358\t\n",
            "\n",
            "2233765\t902240\t925584\t1688080\t\n",
            "3157124\t1766930\t1559950\t2528191\t\n",
            "1395550\t1106832\t867788\t1012104\t\n",
            "2054132\t1219666\t1273836\t1620660\t\n",
            "2109353\t1131308\t852501\t1528793\t\n",
            "\n",
            "642\t868\t920\t473\t936\t\n",
            "647\t629\t613\t112\t602\t\n",
            "138\t831\t98\t640\t809\t\n",
            "68\t734\t124\t933\t137\t\n",
            "846\t599\t345\t467\t81\t\n",
            "\n",
            "429\t940\t902\t661\t\n",
            "128\t236\t279\t996\t\n",
            "133\t753\t908\t780\t\n",
            "358\t497\t892\t960\t\n",
            "636\t699\t34\t252\t\n",
            "\n",
            "1273512\t2390433\t2110356\t2696442\t\n",
            "862572\t1694675\t1436061\t1791515\t\n",
            "922248\t1283201\t1043695\t1813602\t\n",
            "560762\t889980\t1215608\t1802936\t\n",
            "704193\t1485107\t1662791\t1893642\t\n",
            "Your calculations took 0.29 seconds to run.\n",
            "---------------------------==========--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPrHHT0p9uay"
      },
      "source": [
        "THE CODE BELOW EXECUTES THE SAME CALCULATIONS ON CUDA BUT THIS TIME USING 3D BLOCKS AND THREADS. MATRIX A IS 500x500x100 AND MATRIX B = 500x400x100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uSJIS9yx13Q",
        "outputId": "b6cdf2d7-a6d7-41c6-eb3a-096ba47d3c78"
      },
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "\n",
        "__global__ void matrixmul(int *A, int* B, int* C, int dim_n, int dim_k, int quantity){\n",
        "    \n",
        "\n",
        "    //find row and column and the index of the matrix\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int column = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int matrix_index = blockIdx.z*blockDim.z + threadIdx.z;\n",
        "\n",
        "\n",
        "    if( (row < dim_n) && (column < dim_k) && (matrix_index < quantity)){\n",
        "        \n",
        "        int p_sum = 0;\n",
        "        for (int i = 0; i < dim_n; i++){\n",
        "            // \n",
        "            p_sum += A[matrix_index * (dim_n*dim_n) + row * dim_n + i] * B[matrix_index * (dim_n*dim_k) + i * dim_k+ column];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "        C[matrix_index * (dim_n*dim_k) + row * dim_k + column] = p_sum;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "void arrPrintMatrix(int *matrix, int m, int x, int quantity) {\n",
        "    \n",
        "    int i, j;\n",
        "\n",
        "    for (int q = 0; q < quantity; q++){\n",
        "        \n",
        "        for (i = 0; i < m; i++) {\n",
        "            \n",
        "            printf(\"\\n\");\n",
        "            for (j = 0; j < x; j++) {\n",
        "                \n",
        "                printf(\"%d\\t\", matrix[q*(m*x) + i * x + j]);\n",
        "\n",
        "            }\n",
        "\n",
        "        }\n",
        "\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    }\n",
        "\n",
        "    printf(\"------------------------------------------\\n\\n\");\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    clock_t begin = clock();\n",
        "    int quantity = 100;\n",
        "    int n = 500;\n",
        "    int k = 400;\n",
        "\n",
        "    // host matrix m,n,p\n",
        "    int* h_m;\n",
        "    int* h_n;\n",
        "    int* h_p;\n",
        "\n",
        "    //device matrix m,n,p\n",
        "    int* d_m;\n",
        "    int* d_n;\n",
        "    int* d_p;\n",
        "\n",
        "    size_t bytes = n*n*quantity*sizeof(int);\n",
        "\n",
        "    //allocate memory on host side\n",
        "\n",
        "    h_m = (int*)malloc(bytes);\n",
        "    h_n = (int*)malloc(bytes);\n",
        "    h_p = (int*)malloc(bytes);\n",
        "\n",
        "    //initialize matrix A\n",
        "    for (int q = 0; q < quantity; q ++){\n",
        "        \n",
        "        for (int i = 0; i < n; i++){\n",
        "        for (int j = 0; j < n; j++){\n",
        "            h_m[q*(n*n) + i*n + j] = rand() % 1024;\n",
        "\n",
        "        }\n",
        "    }\n",
        "    }\n",
        "    \n",
        "\n",
        "    //initialize matrix B\n",
        "    for (int q = 0; q < quantity; q ++){\n",
        "        for (int i = 0; i < n; i++){\n",
        "        for (int j = 0; j < k; j++){\n",
        "          \n",
        "            h_n[q*(n*k) + i*k + j] = rand() % 1024;\n",
        "\n",
        "        }\n",
        "    }\n",
        "    }\n",
        "  \n",
        "\n",
        "    //arrPrintMatrix(h_m,n,n,quantity);\n",
        "    //arrPrintMatrix(h_n,n,k,quantity);\n",
        "\n",
        "    //allocate memory gpu\n",
        "    cudaMalloc(&d_m, bytes);\n",
        "    cudaMalloc(&d_n, bytes);\n",
        "    cudaMalloc(&d_p, bytes);\n",
        "\n",
        "    cudaMemcpy(d_m, h_m, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_n, h_n, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    int threads_per_block = 128;\n",
        "    dim3 block_size(threads_per_block, threads_per_block, threads_per_block);\n",
        "    dim3 grid_size(n/block_size.x, n/block_size.y, quantity/block_size.z);\n",
        "    matrixmul <<<grid_size, block_size>>>(d_m, d_n, d_p, n,k, quantity);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(h_p, d_p, bytes, cudaMemcpyDeviceToHost);\n",
        "    //arrPrintMatrix(h_p, n,k, quantity);\n",
        "    clock_t end = clock();\n",
        "    double time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
        "    printf (\"Your calculations took %.2lf seconds to run.\\n\", time_spent );\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your calculations took 0.89 seconds to run.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBMLOo3CG0-b"
      },
      "source": [
        "THE EXAMPLE BELOW IS TO PROVE THAT USING THE THREE DIMENSIONS OF THE THREADS AND BLOCKS IS WORKING. I USED IN THIS CASE N = 5 , A is 5x5, AND B IS 5x4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pf6DdLBGzWw",
        "outputId": "198732a4-7be6-4ee3-de0b-3c4282a61fef"
      },
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#include<time.h>\n",
        "\n",
        "__global__ void matrixmul(int *A, int* B, int* C, int dim_n, int dim_k, int quantity){\n",
        "    \n",
        "\n",
        "    //find row and column and the index of the matrix\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int column = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int matrix_index = blockIdx.z*blockDim.z + threadIdx.z;\n",
        "\n",
        "\n",
        "    if( (row < dim_n) && (column < dim_k) && (matrix_index < quantity)){\n",
        "        \n",
        "        int p_sum = 0;\n",
        "        for (int i = 0; i < dim_n; i++){\n",
        "            // \n",
        "            p_sum += A[matrix_index * (dim_n*dim_n) + row * dim_n + i] * B[matrix_index * (dim_n*dim_k) + i * dim_k+ column];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "        C[matrix_index * (dim_n*dim_k) + row * dim_k + column] = p_sum;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "void arrPrintMatrix(int *matrix, int m, int x, int quantity) {\n",
        "    \n",
        "    int i, j;\n",
        "\n",
        "    for (int q = 0; q < quantity; q++){\n",
        "        \n",
        "        for (i = 0; i < m; i++) {\n",
        "            \n",
        "            printf(\"\\n\");\n",
        "            for (j = 0; j < x; j++) {\n",
        "                \n",
        "                printf(\"%d\\t\", matrix[q*(m*x) + i * x + j]);\n",
        "\n",
        "            }\n",
        "\n",
        "        }\n",
        "\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    }\n",
        "\n",
        "    printf(\"------------------------------------------\\n\\n\");\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    clock_t begin = clock();\n",
        "    int quantity = 5;\n",
        "    int n = 5;\n",
        "    int k = 4;\n",
        "\n",
        "    // host matrix m,n,p\n",
        "    int* h_m;\n",
        "    int* h_n;\n",
        "    int* h_p;\n",
        "\n",
        "    //device matrix m,n,p\n",
        "    int* d_m;\n",
        "    int* d_n;\n",
        "    int* d_p;\n",
        "\n",
        "    size_t bytes = n*n*quantity*sizeof(int);\n",
        "\n",
        "    //allocate memory on host side\n",
        "\n",
        "    h_m = (int*)malloc(bytes);\n",
        "    h_n = (int*)malloc(bytes);\n",
        "    h_p = (int*)malloc(bytes);\n",
        "\n",
        "    //initialize matrix A\n",
        "    for (int q = 0; q < quantity; q ++){\n",
        "        \n",
        "        for (int i = 0; i < n; i++){\n",
        "        for (int j = 0; j < n; j++){\n",
        "            h_m[q*(n*n) + i*n + j] = rand() % 1024;\n",
        "\n",
        "        }\n",
        "    }\n",
        "    }\n",
        "    \n",
        "\n",
        "    //initialize matrix B\n",
        "    for (int q = 0; q < quantity; q ++){\n",
        "        for (int i = 0; i < n; i++){\n",
        "        for (int j = 0; j < k; j++){\n",
        "          \n",
        "            h_n[q*(n*k) + i*k + j] = rand() % 1024;\n",
        "\n",
        "        }\n",
        "    }\n",
        "    }\n",
        "  \n",
        "\n",
        "    arrPrintMatrix(h_m,n,n,quantity);\n",
        "    arrPrintMatrix(h_n,n,k,quantity);\n",
        "\n",
        "    //allocate memory gpu\n",
        "    cudaMalloc(&d_m, bytes);\n",
        "    cudaMalloc(&d_n, bytes);\n",
        "    cudaMalloc(&d_p, bytes);\n",
        "\n",
        "    cudaMemcpy(d_m, h_m, bytes, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_n, h_n, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "    int threads_per_block = 1;\n",
        "    dim3 block_size(threads_per_block, threads_per_block, threads_per_block);\n",
        "    dim3 grid_size(n/block_size.x, n/block_size.y, quantity/block_size.z);\n",
        "    matrixmul <<<grid_size, block_size>>>(d_m, d_n, d_p, n,k, quantity);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(h_p, d_p, bytes, cudaMemcpyDeviceToHost);\n",
        "    arrPrintMatrix(h_p, n,k, quantity);\n",
        "    clock_t end = clock();\n",
        "    double time_spent = (double)(end - begin) / CLOCKS_PER_SEC;\n",
        "    printf (\"Your calculations took %.2lf seconds to run.\\n\", time_spent );\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "359\t966\t105\t115\t81\t\n",
            "255\t74\t236\t809\t205\t\n",
            "186\t939\t498\t763\t483\t\n",
            "326\t124\t706\t84\t1016\t\n",
            "795\t488\t487\t909\t886\t\n",
            "\n",
            "346\t302\t611\t563\t927\t\n",
            "201\t922\t870\t306\t13\t\n",
            "951\t561\t88\t163\t346\t\n",
            "293\t349\t261\t791\t88\t\n",
            "745\t94\t212\t427\t178\t\n",
            "\n",
            "205\t198\t667\t692\t84\t\n",
            "529\t14\t386\t116\t577\t\n",
            "289\t317\t476\t135\t624\t\n",
            "489\t62\t161\t577\t225\t\n",
            "508\t871\t574\t769\t638\t\n",
            "\n",
            "663\t490\t732\t875\t918\t\n",
            "911\t56\t92\t554\t748\t\n",
            "176\t59\t763\t562\t175\t\n",
            "316\t852\t492\t792\t987\t\n",
            "92\t258\t26\t254\t835\t\n",
            "\n",
            "251\t762\t682\t826\t507\t\n",
            "297\t465\t998\t5\t316\t\n",
            "892\t916\t373\t984\t446\t\n",
            "97\t137\t505\t860\t699\t\n",
            "680\t153\t527\t149\t945\t\n",
            "------------------------------------------\n",
            "\n",
            "\n",
            "491\t241\t179\t517\t\n",
            "495\t1015\t768\t233\t\n",
            "673\t570\t741\t970\t\n",
            "11\t715\t976\t328\t\n",
            "583\t868\t701\t543\t\n",
            "\n",
            "291\t798\t680\t796\t\n",
            "635\t356\t453\t788\t\n",
            "883\t602\t709\t350\t\n",
            "843\t889\t867\t315\t\n",
            "880\t612\t548\t529\t\n",
            "\n",
            "158\t265\t476\t170\t\n",
            "980\t428\t498\t539\t\n",
            "272\t175\t59\t563\t\n",
            "973\t739\t336\t584\t\n",
            "71\t789\t348\t955\t\n",
            "\n",
            "367\t34\t281\t186\t\n",
            "923\t125\t501\t779\t\n",
            "737\t26\t284\t895\t\n",
            "291\t760\t41\t248\t\n",
            "164\t539\t787\t437\t\n",
            "\n",
            "714\t846\t1000\t664\t\n",
            "562\t312\t224\t633\t\n",
            "77\t573\t564\t444\t\n",
            "607\t846\t631\t506\t\n",
            "971\t108\t261\t684\t\n",
            "------------------------------------------\n",
            "\n",
            "\n",
            "773592\t1279392\t1052975\t594234\t\n",
            "449077\t1027460\t1210642\t754664\t\n",
            "1181267\t2246560\t2206735\t1310542\t\n",
            "1289836\t1548794\t1470932\t1461494\t\n",
            "1486193\t2383488\t2386226\t1776359\t\n",
            "\n",
            "2122338\t1819273\t1801402\t1394970\t\n",
            "1681569\t1292360\t1443602\t1294299\t\n",
            "1152569\t1368249\t1294134\t1464243\t\n",
            "1281594\t1272235\t1276407\t895307\t\n",
            "980282\t1244137\t1167243\t969959\t\n",
            "\n",
            "1087134\t833458\t497281\t1001441\t\n",
            "356129\t754704\t521322\t933573\t\n",
            "661453\t887662\t586026\t1162741\t\n",
            "759210\t788224\t545311\t759034\t\n",
            "1883507\t1679531\t1189840\t1937377\t\n",
            "\n",
            "1640252\t1262626\t1398022\t1778334\t\n",
            "737715\t864578\t921565\t759678\t\n",
            "873622\t554642\t456474\t977433\t\n",
            "1657312\t1263949\t1464617\t1790559\t\n",
            "501914\t679159\t830053\t669251\t\n",
            "\n",
            "1653651\t1594428\t1459869\t1716562\t\n",
            "860105\t1006554\t1049663\t1153339\t\n",
            "2210755\t2134785\t2044866\t2140696\t\n",
            "1385886\t1217223\t1137607\t1288625\t\n",
            "1620123\t1153101\t1352164\t1504131\t\n",
            "------------------------------------------\n",
            "\n",
            "Your calculations took 0.23 seconds to run.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9xekg59JGmo"
      },
      "source": [
        "THE PYTHON EQUIVALENT CODE THAT USES A FOR LOOP TO CALCULATE ONE MATRIX MULTIPLICATION AFTER THE OTHER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iaLoa68dk6b",
        "outputId": "d0a7cf66-13d2-4568-9414-b93fef2dac34"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "###### GENERATING RANDOM MATRICES #######\n",
        "start = time.time()\n",
        "A = np.random.randint(0,1024,(500,500,100))\n",
        "B = np.random.randint(0,1024,(500,400,100))\n",
        "C = []\n",
        "\n",
        "for n in range(100):\n",
        "  matrix_a = A[:,:,n]\n",
        "  matrix_b = B[:,:,n]\n",
        "  result = np.dot(matrix_a, matrix_b)\n",
        "  C.append(result)\n",
        "\n",
        "end = time.time()\n",
        "print(f\"The Python equivalent code took: {end - start} seconds to run.\")\n",
        "# print(end - start)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Python equivalent code took: 33.971593141555786 seconds to run.\n"
          ]
        }
      ]
    }
  ]
}
